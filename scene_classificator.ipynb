{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Scene Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this Deep Learning project focused on scene classification. The primary objective is to build and evaluate Deep Learning models that can accurately classify images into one of the six predefined categories: Buildings, Forests, Glaciers, Mountains, Oceans, and Streets.\n",
    "\n",
    "In this Jupyter Notebook, you'll find:\n",
    "\n",
    "- An exploration of the dataset to better understand its structure and contents.\n",
    "- Pre-processing steps, including image transforms and augmentations, tailored specifically for this dataset.\n",
    "- Implementation of Deep Learning models from scratch to solve the classification problem.\n",
    "- Evaluation of models using metrics like Accuracy, Precision, Recall, and F1 Score.\n",
    "- Visualizations and logs that track model performance during training and validation phases.\n",
    "\n",
    "This project is implemented locally, and Conda is used for package management, ensuring that all dependencies are correctly set up for anyone who wishes to reproduce this work.\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Before diving into the code, it's important to understand the role of each library being imported. Below are the key libraries and their significance in the context of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch is an open-source machine learning library used for a variety of tasks,\n",
    "# but primarily for training deep neural networks.\n",
    "import torch\n",
    "\n",
    "# nn is a sub-module in PyTorch that contains useful classes and functions to build neural networks.\n",
    "import torch.nn as nn\n",
    "\n",
    "# F is a sub-module in PyTorch that contains useful functions for building neural networks.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# DataLoader is a PyTorch utility for loading and batching data efficiently.\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# torchvision contains various utilities, pre-trained models, and datasets specifically\n",
    "# geared towards computer vision tasks.\n",
    "import torchvision\n",
    "\n",
    "# transforms are a set of common image transformations that are often required when\n",
    "# working with image data.\n",
    "from torchvision import transforms\n",
    "\n",
    "# ImageFolder is a utility for loading images directly from a directory structure where\n",
    "# each sub-directory represents a different class.\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# random_split is a utility function to randomly split a dataset into non-overlapping\n",
    "# new datasets of given lengths.\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "# SummaryWriter is a PyTorch utility for logging information to be displayed in TensorBoard.\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# summary is a PyTorch utility for displaying the summary of a PyTorch model.\n",
    "from torchinfo import summary\n",
    "\n",
    "# tqdm is a Python library that adds a progress bar to an iterable object.\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Matplotlib is a plotting library that is useful for visualizing data, plotting graphs, etc.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy is a library for numerical operations and is especially useful for array and\n",
    "# matrix computations.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# PIL is a library for image processing.\n",
    "from PIL import Image\n",
    "\n",
    "# os is a Python module that provides a portable way of using operating system dependent\n",
    "import os\n",
    "\n",
    "# time is a module that provides various time-related functions.\n",
    "import time\n",
    "\n",
    "# random is a module that implements pseudo-random number generators for various distributions.\n",
    "import random\n",
    "\n",
    "# accuracy_score computes the accuracy classification score.\n",
    "# confusion_matrix computes confusion matrix to evaluate the accuracy of a classification.\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# itertools is a module that provides various functions that work on iterators to produce\n",
    "from itertools import product\n",
    "\n",
    "# math is a module that provides access to the mathematical functions.\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking CUDA Availability\n",
    "\n",
    "In deep learning projects, it's common to leverage the power of GPUs for computation. CUDA is a parallel computing platform that allows us to use the GPU for these intensive calculations. The following code snippet checks if CUDA is available on the machine. If CUDA is available, it sets the device to \"cuda\"; otherwise, it falls back to using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the device being used\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Random Seed for Reproducibility\n",
    "\n",
    "For any machine learning experiment, reproducibility is crucial. Setting a random seed ensures that the random numbers generated by our code are the same across different runs, making the results reproducible. In this project, the random seed is set for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 34\n",
    "# Set the seed for generating random numbers\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "TRAIN_DIR = \"data/train_set\"\n",
    "TEST_DIR = \"data/test_set\"\n",
    "CLASSES = os.listdir(TRAIN_DIR)\n",
    "CLASSES_COUNT = len(CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The Exploratory Data Analysis (EDA) phase is an essential step in any data science or machine learning project. The primary objective of EDA is to gain insights into the dataset, understand its complexity, and discover underlying patterns, while also identifying outliers and anomalies that could impact the performance of machine learning models. The information gathered during EDA informs feature engineering, data cleaning, and ultimately, model selection and tuning. \n",
    "\n",
    "This section of the documentation will walk you through various components of the EDA process. From understanding the basic statistics and quality of the data to employing visual techniques for more complex and informative insights, we'll cover it all. \n",
    "\n",
    "Given the image-based nature of the dataset, our EDA will be tailored towards visual data. Specific focus areas will include image size distributions, class distributions, and data quality. By the end of this section, you should have a comprehensive understanding of the data's characteristics and peculiarities, empowering you to make well-informed decisions for the subsequent phases of the project.\n",
    "\n",
    "### Sections in this EDA:\n",
    "\n",
    "1. **Data Overview**: Provides a snapshot of the dataset, including its size and dimensions.\n",
    "2. **Visualization**: Includes various data visualizations to better understand the data's features and labels.\n",
    "3. **Image Characteristics**: Examines the properties of images in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "In this section, we will take an initial look at the dataset to understand its basic characteristics such as size, dimensions, and type of data. Understanding the data's structure is essential for later stages where more specific analyses and modeling will take place.\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- **Total Number of Train Images**: Number of images in the train dataset.\n",
    "- **Class Distribution**: The number of images per class.\n",
    "- **Image Dimensions**: Common or range of dimensions (width x height) among the dataset images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Number of Train Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tain_images = sum([len(files) for subdir, dirs, files in os.walk(TRAIN_DIR)])\n",
    "total_test_images = sum([len(files) for subdir, dirs, files in os.walk(TEST_DIR)])\n",
    "print(f\"Total number of train images: {total_tain_images}\")\n",
    "print(f\"Total number of test images: {total_test_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 14,034 images in our dataset. Given that this is an educational project, this should be more than sufficient for training a robust model. Large datasets are generally beneficial for deep learning models, providing them with more opportunities to learn nuanced features across different classes. Therefore, data scarcity is not a concern for us in this particular project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = os.listdir(TRAIN_DIR)\n",
    "label_counter = {}\n",
    "for folder in train_labels:\n",
    "    folder_path = os.path.join(TRAIN_DIR, folder)\n",
    "    num_files = len(os.listdir(folder_path))\n",
    "    label_counter[folder] = num_files\n",
    "\n",
    "classes = list(label_counter.keys())\n",
    "count = list(label_counter.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(classes, count, color=\"blue\")\n",
    "\n",
    "plt.xlabel(\"Number of Images\")\n",
    "plt.ylabel(\"Class Label\")\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset exhibits a balanced distribution across different classes, which is advantageous for the learning process. This removes the need for techniques such as class balancing during training, simplifying the model development process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Dimensions\n",
    "In this section, we aim to explore the dimensions of the images in our dataset. Understanding the size and format of the images can provide insights into the preprocessing steps needed, as well as help in designing the architecture of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sizes = []\n",
    "\n",
    "for class_folder in os.listdir(TRAIN_DIR):\n",
    "    class_folder_path = os.path.join(TRAIN_DIR, class_folder)\n",
    "\n",
    "    if os.path.isdir(class_folder_path):\n",
    "        for image_name in os.listdir(class_folder_path):\n",
    "            image_path = os.path.join(class_folder_path, image_name)\n",
    "\n",
    "            with Image.open(image_path) as img:\n",
    "                width, height = img.size\n",
    "                image_sizes.append((width, height))\n",
    "\n",
    "# Compute some statistics\n",
    "avg_size = np.mean(image_sizes, axis=0)\n",
    "min_size = np.min(image_sizes, axis=0)\n",
    "max_size = np.max(image_sizes, axis=0)\n",
    "\n",
    "print(f\"Average Size: {avg_size}\")\n",
    "print(f\"Minimum Size: {min_size}\")\n",
    "print(f\"Maximum Size: {max_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that not all images have the same dimensions (150x150). Therefore, when loading the images, we will apply the corresponding transformation to standardize their size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atypical_images = []\n",
    "\n",
    "for class_folder in os.listdir(TRAIN_DIR):\n",
    "    class_folder_path = os.path.join(TRAIN_DIR, class_folder)\n",
    "\n",
    "    if os.path.isdir(class_folder_path):\n",
    "        for image_name in os.listdir(class_folder_path):\n",
    "            image_path = os.path.join(class_folder_path, image_name)\n",
    "\n",
    "            with Image.open(image_path) as img:\n",
    "                width, height = img.size\n",
    "                if width != 150 or height != 150:\n",
    "                    atypical_images.append((image_path, (width, height)))\n",
    "\n",
    "\n",
    "# Display the atypical images\n",
    "if atypical_images:\n",
    "    print(f\"Atypical image sizes found({len(atypical_images)}):\")\n",
    "    for img_path, size in atypical_images:\n",
    "        print(f\"{img_path}: {size}\")\n",
    "else:\n",
    "    print(\"No atypical images found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_classes = random.sample(CLASSES, 4)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "for idx, random_class in enumerate(random_classes):\n",
    "    # Obtener una lista de todas las imágenes en una clase específica\n",
    "    img_list = os.listdir(os.path.join(TRAIN_DIR, random_class))\n",
    "\n",
    "    # Seleccionar una imagen al azar de la lista\n",
    "    random_img_name = random.choice(img_list)\n",
    "\n",
    "    # Ruta a la imagen\n",
    "    img_path = os.path.join(TRAIN_DIR, random_class, random_img_name)\n",
    "\n",
    "    # Cargar la imagen\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    # Mostrar la imagen\n",
    "    plt.subplot(2, 2, idx + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Clase: {random_class}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Loading\n",
    "\n",
    "In this section, we deal with the vital aspect of preparing our data. These steps can significantly influence the model's performance.\n",
    "\n",
    "- **Data Loading**: The ImageFolder class from torchvision is used to load our dataset from the disk.\n",
    "- **Transformations**: The dataset is transformed to standardize the image size and apply data augmentation techniques such as random horizontal flips and random rotations.\n",
    "- **Data Splitting**: The dataset is split into training and validation sets. 80% of the data is used for training, and the remaining 20% is used for validation.\n",
    "- **DataLoader**: Finally, PyTorch's DataLoader is used to create mini-batches of data, which allows for more efficient model training.\n",
    "\n",
    "By the end of this section, we have DataLoader instances for the training, validation, and testing sets, which can be used to train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(image_size, batch_size):\n",
    "    \n",
    "    # Create transforms for data augmentation\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.RandomVerticalFlip(p=0.2),\n",
    "            transforms.ToTensor(),  # Convert PIL Image to PyTorch tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),  # Convert PIL Image to PyTorch tensor\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Load the training and test datasets from disk\n",
    "    train_dataset = ImageFolder(\"data/train_set\", transform=train_transform)\n",
    "    test_dataset = ImageFolder(\"data/test_set\", transform=test_transform)\n",
    "\n",
    "    # Split the dataset into training and validation sets\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    valid_size = len(train_dataset) - train_size\n",
    "    train_subset, validation_subset = random_split(train_dataset, [train_size, valid_size])\n",
    "\n",
    "    # Create DataLoader instances to load data in batches\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "    val_loader = DataLoader(validation_subset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters Used\n",
    "This project involves training neural network models with various combinations of hyperparameters to identify the most effective setup for a given task. The main script includes functions for model training, monitoring, and evaluation, and utilizes different sets of hyperparameters for extensive experimentation.\n",
    "\n",
    "The hyperparameters under consideration include the learning rate, batch size, type of optimizer, image size for data preprocessing, and the number of epochs for training.\n",
    "\n",
    "The loss function used for the training is Cross-Entropy Loss. It is initialized and moved to the appropriate device (CPU or GPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATES = [0.01, 0.001, 0.0001, ]\n",
    "BATCH_SIZES = [32, 64, 128]\n",
    "OPTMIZERS = ['SGD', 'Adam']\n",
    "IMAGES_SIZES = [32, 64, 128]\n",
    "EPOCHS = [30, 100]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions for Model Training and Evaluation\n",
    "\n",
    "In this section, we present a set of utility functions designed to facilitate the training, monitoring, and evaluation processes for various neural network architectures. By creating these generic functions, we aim to make the process of experimenting with different architectures more streamlined and comparable.\n",
    "\n",
    "### Monitoring Function\n",
    "\n",
    "This function assists in keeping track of important metrics like loss and accuracy during the training and validation phases. The function uses TensorBoard's SummaryWriter to log these metrics for easy visualization.\n",
    "\n",
    "```python\n",
    "def monitor_metrics(writer, epoch_num, loss, accuracy, phase):\n",
    "```\n",
    "\n",
    "### Training Function\n",
    "\n",
    "This function handles the training process for each epoch, iterating over batches of data and updating the model parameters.\n",
    "\n",
    "```python\n",
    "def train_model(model, criterion, optimizer, train_loader, num_epochs=10):\n",
    "```\n",
    "\n",
    "### Running an Epoch\n",
    "\n",
    "This function is called in each epoch during the training and evaluation phases. It performs forward and backward passes and computes loss and accuracy for the epoch.\n",
    "\n",
    "```python\n",
    "def run_epoch(epoch_num, model, loader, criterion, writer, optim=None, do_logging=False, is_training=True):\n",
    "```\n",
    "\n",
    "### Hyperparameter Tuning Function\n",
    "\n",
    "This function is responsible for finding the best model given a set of hyperparameters. It trains models for all possible combinations of learning rates, batch sizes, optimizer types, image sizes, and epochs, and saves the best-performing model based on accuracy.\n",
    "\n",
    "The `model_generator` functions a crucial role in the hyperparameter tuning process. It is responsible for generating a fresh instance of the neural network model based on certain parameters, such as `image_size`. This allows us to experiment with different model architectures and hyperparameters.\n",
    "\n",
    "```python\n",
    "def find_best_model(model_generator):\n",
    "```\n",
    "\n",
    "### Evaluation Function\n",
    "\n",
    "After training, this function evaluates the performance of the model on a test dataset and generates useful metrics such as precision, recall, and F1-score.\n",
    "\n",
    "```python\n",
    "def evaluate_model(model, test_loader):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_metrics(writer, epoch_num, loss, accuracy, phase):\n",
    "    writer.add_scalar(f'{phase} loss', loss, epoch_num)\n",
    "    writer.add_scalar(f'{phase} accuracy', accuracy, epoch_num)\n",
    "\n",
    "def run_epoch(epoch_num, model, loader, criterion, writer, optim=None, do_logging=False, is_training=True):\n",
    "    if is_training:\n",
    "        model.train()\n",
    "        epoch_type = \"Training\"\n",
    "    else:\n",
    "        model.eval()\n",
    "        epoch_type = \"Validation\"\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.set_grad_enabled(\n",
    "        is_training\n",
    "    ):  # set gradient calculation to True or False depending on mode\n",
    "        for images, labels in loader:\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "            if is_training:\n",
    "                optim.zero_grad()  # reset the gradients to 0 for all learnable parameters\n",
    "\n",
    "            predictions = model(images.to(device))  # forward pass\n",
    "            all_predictions.extend(\n",
    "                torch.argmax(predictions, dim=1).cpu().numpy()\n",
    "            )  # get the predicted class with highest probability\n",
    "\n",
    "            loss = criterion(predictions, labels.to(device))  # compute the loss\n",
    "\n",
    "            if is_training:\n",
    "                loss.backward()  # compute the gradients for each learnable parameter\n",
    "                optim.step()  # update the weights\n",
    "\n",
    "            epoch_loss += loss.item()  # accumulate the loss for each batch\n",
    "\n",
    "    avg_loss = epoch_loss / len(loader)  # compute the average loss for the epoch\n",
    "    accuracy = (\n",
    "        accuracy_score(all_labels, all_predictions) * 100\n",
    "    )  # compute the accuracy for the epoch\n",
    "\n",
    "    if do_logging:\n",
    "        monitor_metrics(writer, epoch_num, avg_loss, accuracy, epoch_type)\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optim, number_epochs, do_logging = False, logging_name = \"log\", patience=10\n",
    "):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    writer = None\n",
    "\n",
    "    if do_logging:\n",
    "        writer = SummaryWriter('runs/' + model.name + \"/\" + logging_name)\n",
    "\n",
    "    for epoch in tqdm(range(number_epochs)):\n",
    "        # Train the model for one epoch\n",
    "        run_epoch(\n",
    "            epoch + 1, model, train_loader, criterion, writer, optim, do_logging, is_training=True\n",
    "        )\n",
    "\n",
    "        # Evaluate the model against validation set\n",
    "        test_loss, _ = run_epoch(\n",
    "            epoch + 1, model, val_loader, criterion, writer, do_logging, is_training=False\n",
    "        )\n",
    "\n",
    "        \n",
    "        # early stopping: if the validation loss does not decrease for 10 consecutive epochs, stop training\n",
    "        if test_loss < best_val_loss:\n",
    "            best_val_loss = test_loss\n",
    "            counter = 0  # Restablecer el contador\n",
    "        else:\n",
    "            counter += 1  # Incrementar el contador\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "\n",
    "def train_model_with_hyperparameters(model_generator, lr, batch_size, optimizer_type, image_size, number_of_epochs):\n",
    "    torch.manual_seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "    print(f\"Training model with lr={lr}, batch_size={batch_size}, optimizer={optimizer_type}, image_size={image_size}, epoch={number_of_epochs}\")\n",
    "\n",
    "    train_loader, valid_loader, _ = get_dataloaders(\n",
    "        image_size, batch_size\n",
    "    )\n",
    "\n",
    "    # Create the model\n",
    "    model = model_generator(image_size).to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    if optimizer_type == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    elif optimizer_type == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise Exception('Invalid optimizer type')\n",
    "    \n",
    "    logging_name = f\"{model.name}-lr={lr}-ba={batch_size}-opt={optimizer_type}-img={image_size}\"\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, train_loader, valid_loader, criterion, optimizer, number_of_epochs, True, logging_name)\n",
    "    # Evaluate the model\n",
    "    model_accuracy, _, _, _, _, _ = evaluate_model(model, valid_loader)\n",
    "\n",
    "    return model, model_accuracy\n",
    "\n",
    "\n",
    "def find_best_model(model_generator):\n",
    "    best_accuracy = 0.0\n",
    "    best_params = {}\n",
    "    best_model = None\n",
    "\n",
    "\n",
    "    # use itertools.product to get all possible combinations of hyperparameters\n",
    "    all_combinations = list(product(LEARNING_RATES, BATCH_SIZES, OPTMIZERS, IMAGES_SIZES, EPOCHS))\n",
    "    current_combination = 0\n",
    "    total_combination_runs = len(all_combinations)\n",
    "\n",
    "    # Iterate over all combinations\n",
    "    for combination in all_combinations:\n",
    "        lr, batch_size, optimizer_type, image_size, number_of_epochs = combination\n",
    "\n",
    "        current_combination += 1\n",
    "        print(f\"Running combination {current_combination}/{total_combination_runs}\")\n",
    "        model, model_accuracy = train_model_with_hyperparameters(model_generator, lr, batch_size, optimizer_type, image_size, number_of_epochs)\n",
    "\n",
    "        \n",
    "        print(f\"Accuracy: {model_accuracy:.2f}%\")\n",
    "\n",
    "        # Free up GPU memory\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Save the best hyperparameters\n",
    "        if model_accuracy > best_accuracy:\n",
    "            best_accuracy = model_accuracy\n",
    "            best_params = {\n",
    "                \"lr\": lr,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"optimizer_type\": optimizer_type,\n",
    "                \"image_size\": image_size,\n",
    "                \"number_of_epochs\": number_of_epochs,\n",
    "            }\n",
    "            best_model = model\n",
    "            \n",
    "    print(f\"Best accuracy: {best_accuracy}\")\n",
    "    print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "    return best_model, best_params\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, (images, labels) in enumerate(test_loader):\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "            predictions = model(images.to(device))  # forward pass\n",
    "            all_predictions.extend(\n",
    "                torch.argmax(predictions, dim=1).cpu().numpy()\n",
    "            )  # get the predicted class with highest probability\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions) * 100\n",
    "    precision = precision_score(all_labels, all_predictions, average=\"macro\")\n",
    "    recall = recall_score(all_labels, all_predictions, average=\"macro\")\n",
    "    f1 = f1_score(all_labels, all_predictions, average=\"macro\")\n",
    "\n",
    "    return accuracy, precision, recall, f1, all_labels, all_predictions\n",
    "\n",
    "def print_metrics(accuracy, precision, recall, f1, labels, predicions):\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1: {f1:.2f}\")\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(labels, predicions)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Solutions Overview\n",
    "\n",
    "In this project, we aim to build and evaluate various neural network architectures to solve our specific classification problem. We will approach this task by employing three distinct architectures, each with its unique characteristics and advantages:\n",
    "\n",
    "### Baseline Model\n",
    "Our baseline model will be a simple Convolutional Neural Network (CNN) with a minimal number of layers. This model will consist of a few convolutional layers followed by pooling layers and fully connected layers towards the end. The purpose of establishing a baseline is to have a point of reference against which we can measure the performance of more complex models.\n",
    "\n",
    "### ResNet Architecture\n",
    "As our second model, we will use the [ResNet (Residual Network)](https://arxiv.org/pdf/1512.03385.pdf) architecture, which is well-known for its excellent performance in image classification tasks. ResNet introduces \"skip connections\" that bypass one or more layers, allowing for deeper networks without the problem of vanishing gradients. \n",
    "\n",
    "### DenseNet Architecture\n",
    "Lastly, we will use [DenseNet (Densely Connected Convolutional Networks)](https://arxiv.org/pdf/1608.06993.pdf) for our third model. This architecture improves upon ResNet by connecting each layer to every other layer in a feed-forward fashion, thereby increasing computational efficiency and enhancing feature propagation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Modalities\n",
    "\n",
    "### Data Splitting\n",
    "The dataset will be partitioned into three sets: training, validation, and testing sets. \n",
    "\n",
    "### Hyperparameter Tuning\n",
    "We will carry out multiple runs with different hyperparameters, using the validation set as our benchmark. After tuning the models to satisfaction on the validation set, they will be locked, and final evaluations will be performed on the test set.\n",
    "\n",
    "### Evaluation Metrics\n",
    "To evaluate the models, we will use a range of metrics suitable for multi-class classification problems. These include:\n",
    "- `accuracy_score`: Overall accuracy of the model.\n",
    "- `confusion_matrix`: To understand the classification errors.\n",
    "- `precision_score`: Measures the accuracy of positive predictions.\n",
    "- `recall_score`: Measures the ability to find all positive instances.\n",
    "- `f1_score`: Harmonic mean of precision and recall.\n",
    "\n",
    "By employing these metrics, we aim to provide a comprehensive evaluation of each model's performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleCNN: The Baseline Model\n",
    "\n",
    "### Introduction\n",
    "SimpleCNN is designed to serve as the baseline model for our image classification problem. It is a straightforward convolutional neural network (CNN) that captures essential features from images while remaining computationally inexpensive. This makes it an ideal starting point to understand the basic performance metrics we can achieve and sets the stage for comparison with more complex architectures.\n",
    "\n",
    "### Architecture\n",
    "The architecture is uncomplicated, consisting of three main blocks:\n",
    "\n",
    "#### Convolutional Layers\n",
    "- **Conv1**: A 3x3 convolutional layer with 16 filters, followed by a ReLU activation.\n",
    "- **Conv2**: Another 3x3 convolutional layer with 32 filters, followed by a ReLU activation.\n",
    "- **Conv3**: A final 3x3 convolutional layer with 64 filters, also followed by a ReLU activation.\n",
    "\n",
    "Each convolutional layer is accompanied by a max-pooling layer with a size of 2x2, which reduces the dimensions of the image while keeping the important features.\n",
    "\n",
    "#### Fully Connected Layers\n",
    "- **FC1**: A fully connected layer with a ReLU activation that outputs to 512 units.\n",
    "- **FC2**: Another fully connected layer that outputs to the number of classes in the dataset.\n",
    "\n",
    "### Forward Propagation\n",
    "The forward propagation steps are intuitive and simple to follow:\n",
    "1. The image first passes through the three convolutional layers (`conv1`, `conv2`, `conv3`), each followed by a ReLU activation and max-pooling.\n",
    "2. The output is then flattened and passed through the first fully connected layer (`fc1`) with a ReLU activation.\n",
    "3. Finally, the output goes through the last fully connected layer (`fc2`) to produce the class scores.\n",
    "\n",
    "### Why SimpleCNN?\n",
    "SimpleCNN serves as a no-frills approach to understanding what the bare minimum architecture can achieve. It sets the stage for implementing and comparing more advanced models like ResNet and DenseNet. Although SimpleCNN may not produce state-of-the-art results, it gives us a valuable point of reference for gauging the performance of subsequent architectures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes, img_size):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.name = \"SimpleCNN\"\n",
    "        \n",
    "        fc_size = img_size // 8\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * fc_size * fc_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATES = [0.01, 0.001, 0.0001]\n",
    "BATCH_SIZES = [64, 128, 256]\n",
    "OPTMIZERS = ['SGD', 'Adam']\n",
    "IMAGES_SIZES = [32, 64, 128]\n",
    "EPOCHS = [100]\n",
    "\n",
    "# this function generates a simple CNN model with the given image size\n",
    "def generate_simple_cnn_model(image_size):\n",
    "    return SimpleCNN(CLASSES_COUNT, image_size)\n",
    "\n",
    "# find the best model using different hyperparameters combination and keep the best one (based on accuracy of validation set)\n",
    "best_simple_cnn_model, best_params = find_best_model(generate_simple_cnn_model)\n",
    "\n",
    "# display the summary of the best model (architecture, parameters, etc.)\n",
    "summary(best_simple_cnn_model, input_size=(best_params['batch_size'], 3, best_params['image_size'], best_params['image_size']))\n",
    "\n",
    "# load the test dataset\n",
    "_, _, test_loader = get_dataloaders(\n",
    "        best_params['image_size'], 128\n",
    "    )\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "evaluate_model(best_simple_cnn_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs/SimpleCNN --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet: Advanced Model for Robust Feature Learning\n",
    "\n",
    "### Introduction\n",
    "The ResNet architecture is designed for deep learning tasks where the network depth is crucial for capturing intricate patterns. ResNet employs residual blocks that allow the network to learn from the residual error, which enables the training of very deep networks without the hindrance of vanishing gradients. This architecture is often used for challenging tasks in both image and natural language processing domains.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "#### Convolutional Layer and Batch Normalization\n",
    "- **Conv1**: A 3x3 convolutional layer with 64 filters, with stride 1 and padding 1. Followed by Batch Normalization and ReLU activation.\n",
    "\n",
    "#### Residual Blocks\n",
    "Residual Blocks are the heart of the ResNet model. Each block contains:\n",
    "- **Conv1**: A 3x3 convolutional layer.\n",
    "- **BatchNorm1**: Followed by Batch Normalization.\n",
    "- **Conv2**: Another 3x3 convolutional layer.\n",
    "- **BatchNorm2**: Followed by Batch Normalization.\n",
    "- **Shortcut**: A shortcut connection that can bypass one or more layers during the forward and backward passes.\n",
    "\n",
    "ResNet uses several such residual blocks and groups them into four layers:\n",
    "- **Layer 1**: Consisting of `num_blocks[0]` residual blocks with 64 filters.\n",
    "- **Layer 2**: Consisting of `num_blocks[1]` residual blocks with 128 filters.\n",
    "- **Layer 3**: Consisting of `num_blocks[2]` residual blocks with 256 filters.\n",
    "- **Layer 4**: Consisting of `num_blocks[3]` residual blocks with 512 filters.\n",
    "\n",
    "Each layer may change the dimensions of its input tensor, typically by down-sampling the spatial dimensions.\n",
    "\n",
    "#### Fully Connected Layer\n",
    "- **Linear**: A fully connected layer that outputs to the number of classes. The input dimension is dynamically calculated based on the image size and other architectural details.\n",
    "\n",
    "### Forward Propagation\n",
    "1. The input image first passes through an initial convolutional layer (`conv1`), followed by Batch Normalization and ReLU activation.\n",
    "2. The image then goes through four layers of residual blocks (`layer1`, `layer2`, `layer3`, `layer4`), each having a different number of blocks and filters.\n",
    "3. Finally, an average pooling layer condenses the feature maps.\n",
    "4. The output is flattened and passed through the fully connected layer to produce the class scores.\n",
    "\n",
    "### Why ResNet?\n",
    "The ResNet architecture's main advantage is its ability to train extremely deep networks by leveraging residual learning. It has proven to be effective in various applications and has set multiple benchmarks in different challenges. It's particularly useful for tasks requiring the model to learn from highly complex and nuanced data.\n",
    "\n",
    "This robustness makes it an excellent choice for developers and researchers who are looking to push the boundaries of what's achievable with current deep learning technologies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # First convolutional layer in the residual block\n",
    "        # Followed by Batch Normalization\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Second convolutional layer in the residual block\n",
    "        # Followed by Batch Normalization\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut connection to match dimensions\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=1, stride=stride, bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        out = F.relu(self.bn1(self.conv1(x)))  # First Conv -> BN -> ReLU\n",
    "        out = self.bn2(self.conv2(out))  # Second Conv -> BN\n",
    "        out += self.shortcut(x)  # Add the shortcut\n",
    "        out = F.relu(out)  # Final ReLU\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, img_size=32):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.name = \"ResNet\"\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.feature_size = img_size // 32\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        last_layer_output = 512 * ((img_size * img_size * 8) // (512 * self.feature_size * self.feature_size))\n",
    "\n",
    "        # Modify the input dimension for the fully connected layer\n",
    "        self.linear = nn.Linear(last_layer_output, 512)\n",
    "        self.linear2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, self.feature_size)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.linear(out))\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATES = [0.001, 0.0001]\n",
    "BATCH_SIZES = [32, 64]\n",
    "OPTMIZERS = ['SGD', 'Adam']\n",
    "IMAGES_SIZES = [32, 64]\n",
    "EPOCHS = [100]\n",
    "\n",
    "# this function generates a ResNet model with the given image size\n",
    "def generate_resnet_model(image_size):\n",
    "    return ResNet(ResidualBlock, [2, 2, 2, 2], CLASSES_COUNT, image_size)\n",
    "\n",
    "# find the best model using different hyperparameters combination and keep the best one (based on accuracy of validation set)\n",
    "best_resnet_model, best_params = find_best_model(generate_resnet_model)\n",
    "\n",
    "# display the summary of the best model (architecture, parameters, etc.)\n",
    "summary(best_resnet_model, input_size=(best_params['batch_size'], 3, best_params['image_size'], best_params['image_size']))\n",
    "\n",
    "# load the test dataset\n",
    "_, _, test_loader = get_dataloaders(\n",
    "        best_params['image_size'], 128\n",
    "    )\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "evaluate_model(best_resnet_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs/ResNet --port=6007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet: Highly Efficient and Compact Architecture for Deep Learning\n",
    "\n",
    "### Introduction\n",
    "DenseNet, or Densely Connected Convolutional Networks, is designed to optimize the flow of information and gradients between layers in deep neural networks. It accomplishes this by connecting each layer's input to the outputs of all preceding layers, ensuring maximum information flow. This architecture is highly efficient and has shown excellent performance in tasks like image classification.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "#### Initial Convolutional Layer\n",
    "- **init_conv**: An initial 7x7 convolutional layer with 64 filters, a stride of 2, and padding of 3. This is followed by Batch Normalization and a ReLU activation function.\n",
    "\n",
    "#### Dense Blocks\n",
    "- **DenseBlock**: A key component of DenseNet. It comprises multiple `ConvLayer` units where each unit outputs feature-maps that are used as input for all subsequent layers within the block.\n",
    "\n",
    "#### Transition Layers\n",
    "- **TransitionLayer**: These are interspersed between Dense Blocks and reduce the dimensions of the feature maps, helping to control the model's complexity.\n",
    "\n",
    "#### Fully Connected Layer\n",
    "- **fc**: A fully connected layer with 512 neurons, followed by another fully connected layer that outputs the number of classes. \n",
    "\n",
    "### Forward Propagation\n",
    "1. The input passes through the `init_conv` layer for initial feature extraction.\n",
    "2. The processed input is then fed through a sequence of Dense Blocks and Transition Layers: `dense1 -> trans1 -> dense2 -> trans2 -> dense3 -> trans3 -> dense4`.\n",
    "3. Adaptive Average Pooling condenses the final feature maps into a single vector.\n",
    "4. This vector is passed through the fully connected layer (`fc`) to output class scores.\n",
    "\n",
    "### Why DenseNet?\n",
    "DenseNet's unique architecture makes it very parameter-efficient, mitigates the vanishing gradient problem, and encourages feature reuse, making it a strong choice for various computer vision tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "    \n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, num_layers):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(ConvLayer(in_channels + i * growth_rate, growth_rate))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = [x]\n",
    "        for layer in self.layers:\n",
    "            out = layer(torch.cat(outputs, dim=1))\n",
    "            outputs.append(out)\n",
    "        return torch.cat(outputs, dim=1)\n",
    "    \n",
    "class TransitionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(TransitionLayer, self).__init__()\n",
    "        self.conv = ConvLayer(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.pool = nn.AvgPool2d(2, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.name = \"DenseNet\"\n",
    "        self.input_size = input_size\n",
    "        self.init_conv = ConvLayer(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.pool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        \n",
    "        \n",
    "        self.dense1 = DenseBlock(64, 32, 4)\n",
    "        self.trans1 = TransitionLayer(192, 96)  # 64 + 4 * 32 = 192 | 192 / 2 = 96\n",
    "        self.dense2 = DenseBlock(96, 32, 4)\n",
    "        self.trans2 = TransitionLayer(224, 112)  # 96 + 4 * 32 = 224 | 224 / 2 = 112\n",
    "        self.dense3 = DenseBlock(112, 32, 4)\n",
    "        self.trans3 = TransitionLayer(240, 120)  # 112 + 4 * 32 = 240 | 240 / 2 = 120\n",
    "        self.dense4 = DenseBlock(120, 32, 4)\n",
    "        \n",
    "        self.fc = nn.Linear(248, 512)  # 120 + 4 * 32 = 248\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.init_conv(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.dense1(x)\n",
    "        x = self.trans1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.trans2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.trans3(x)\n",
    "        x = self.dense4(x)\n",
    "        \n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATES = [0.001, 0.0001]\n",
    "BATCH_SIZES = [128, 256]\n",
    "OPTMIZERS = ['SGD', 'Adam']\n",
    "IMAGES_SIZES = [64, 128]\n",
    "EPOCHS = [100]\n",
    "\n",
    "# this function generates a ResNet model with the given image size\n",
    "def generate_densenet_model(image_size):\n",
    "    return DenseNet(CLASSES_COUNT, image_size)\n",
    "\n",
    "# find the best model using different hyperparameters combination and keep the best one (based on accuracy of validation set)\n",
    "best_densenet_model, best_params = find_best_model(generate_densenet_model)\n",
    "\n",
    "# display the summary of the best model (architecture, parameters, etc.)\n",
    "summary(best_densenet_model, input_size=(best_params['batch_size'], 3, best_params['image_size'], best_params['image_size']))\n",
    "\n",
    "# load the test dataset\n",
    "_, _, test_loader = get_dataloaders(\n",
    "        best_params['image_size'], 128\n",
    "    )\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "evaluate_model(best_densenet_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs/DenseNet --port=6008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs --port=6009"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scene_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
